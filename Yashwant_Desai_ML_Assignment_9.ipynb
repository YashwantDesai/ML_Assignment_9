{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c27a818",
   "metadata": {},
   "source": [
    "# Yashwant Desai –  ML_Assignment_9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00df5172",
   "metadata": {},
   "source": [
    "# 1. What is feature engineering, and how does it work? Explain the various aspects of feature engineering in depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af411efc",
   "metadata": {},
   "source": [
    "Feature engineering is the process of creating new features or modifying existing ones in a dataset to improve the performance of machine learning models. \n",
    "\n",
    "It involves:\n",
    "\n",
    "Feature Creation: Generating new features from existing ones. For example, creating a \"total income\" feature by adding \"salary\" and \"bonus.\"\n",
    "\n",
    "Feature Transformation: Modifying features through mathematical operations, such as taking the logarithm of a variable to make its distribution more Gaussian.\n",
    "\n",
    "Feature Selection: Choosing the most relevant features to reduce dimensionality.\n",
    "\n",
    "Handling Missing Data: Dealing with missing values in features using techniques like imputation.\n",
    "\n",
    "Encoding Categorical Variables: Converting categorical variables into numerical format, e.g., one-hot encoding.\n",
    "\n",
    "Scaling and Normalization: Scaling features to have similar scales, e.g., standardization.\n",
    "\n",
    "Feature Extraction: Creating new features through techniques like Principal Component Analysis (PCA)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a5cdc7",
   "metadata": {},
   "source": [
    "# 2. What is feature selection, and how does it work? What is the aim of it? What are the various methods of function selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a81907a",
   "metadata": {},
   "source": [
    "Feature selection is the process of choosing a subset of the most relevant features from a dataset to improve model performance, reduce dimensionality, and decrease computational cost. The aim is to retain the most informative features. \n",
    "\n",
    "Methods of feature selection include:\n",
    "\n",
    "Filter Methods: Use statistical tests to score and rank features based on their individual relationship with the target variable (e.g., Chi-squared, Information Gain).\n",
    "\n",
    "Wrapper Methods: Evaluate subsets of features by training and testing the model on different combinations (e.g., Forward Selection, Backward Elimination).\n",
    "\n",
    "Embedded Methods: Feature selection is integrated into the model training process (e.g., LASSO for linear regression, decision tree feature importance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e278fc",
   "metadata": {},
   "source": [
    "# 3. Describe the function selection filter and wrapper approaches. State the pros and cons of each approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d518b70",
   "metadata": {},
   "source": [
    "Filter Approach: In filter methods features are ranked independently of the machine learning model. \n",
    "\n",
    "Pros include efficiency and speed but cons involve potentially missing feature interactions and dependencies on the selected model.\n",
    "\n",
    "Wrapper Approach: In wrapper methods features are evaluated in the context of the specific machine learning model being used. \n",
    "\n",
    "Pros include capturing feature interactions but cons involve high computational cost and potential overfitting to the model used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf466bf",
   "metadata": {},
   "source": [
    "# 4. i. Describe the overall feature selection process. ii. Explain the key underlying principle of feature extraction using an example. What are the most widely used function extraction algorithms?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7c5c6a",
   "metadata": {},
   "source": [
    "The overall feature selection process. The feature selection process typically involves three main steps:\n",
    "\n",
    "1 Feature Ranking: Features are scored based on their relevance to the target variable.\n",
    "\n",
    "2 Subset Selection: A subset of top-ranked features is chosen based on a predefined criterion (e.g., number of features to select).\n",
    "\n",
    "3 Model Evaluation: The selected subset of features is used to train and test a machine learning model, and its performance is assessed.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aba720",
   "metadata": {},
   "source": [
    "Below is the key underlying principle of feature extraction\n",
    "\n",
    "Feature extraction transforms high-dimensional data into a lower-dimensional representation. \n",
    "\n",
    "For example, Principal Component Analysis (PCA) identifies the most important orthogonal directions (principal components) in the data and projects it onto a lower-dimensional space while preserving the most variance. Widely used feature extraction algorithms include PCA, Linear Discriminant Analysis (LDA) and Independent Component Analysis (ICA)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731104e6",
   "metadata": {},
   "source": [
    "# 5. Describe the feature engineering process in the sense of a text categorization issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f31b9da",
   "metadata": {},
   "source": [
    "In text categorization feature engineering involves converting text data into numerical features. It can include methods like TF-IDF (Term Frequency-Inverse Document Frequency), word embeddings and creating features based on the presence of specific words or phrases in documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e98826",
   "metadata": {},
   "source": [
    "# 6. What makes cosine similarity a good metric for text categorization? A document-term matrix has two rows with values of (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the resemblance in cosine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ab580cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity between A and B: 0.6753032524419089\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the two vectors A and B\n",
    "A = np.array([2, 3, 2, 0, 2, 3, 3, 0, 1])\n",
    "B = np.array([2, 1, 0, 0, 3, 2, 1, 3, 1])\n",
    "\n",
    "# Calculate the dot product of A and B\n",
    "dot_product = np.dot(A, B)\n",
    "\n",
    "# Calculate the Euclidean norms (magnitudes) of A and B\n",
    "norm_A = np.linalg.norm(A)\n",
    "norm_B = np.linalg.norm(B)\n",
    "\n",
    "# Calculate the cosine similarity\n",
    "cosine_similarity = dot_product / (norm_A * norm_B)\n",
    "\n",
    "print(f\"Cosine Similarity between A and B: {cosine_similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d5ac1c",
   "metadata": {},
   "source": [
    "Cosine similarity is a good metric for text categorization because it measures the cosine of the angle between two vectors which represents the similarity of their directions in a high-dimensional space. \n",
    "\n",
    "To calculate cosine similarity between the two rows:\n",
    "\n",
    "Calculate the dot product of the two vectors: (2 * 2) + (3 * 1) + (2 * 0) + (0 * 0) + (2 * 3) + (3 * 2) + (3 * 1) + (0 * 3) + (1 * 1) = 14\n",
    "\n",
    "Calculate the magnitude of each vector: sqrt((2^2 + 3^2 + 2^2 + 0^2 + 2^2 + 3^2 + 3^2 + 0^2 + 1^2)) = sqrt(36) = 6\n",
    "\n",
    "Calculate cosine similarity: (14) / (6 * 6) = 14 / 36 = 7 / 18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df1f2c9",
   "metadata": {},
   "source": [
    "# 7. i. What is the formula for calculating Hamming distance? Between 10001011 and 11001111, calculate the Hamming gap. ii. Compare the Jaccard index and similarity matching coefficient of two features with values (1, 1, 0, 0, 1, 0, 1, 1) and (1, 1, 0, 0, 0, 1, 1, 1), respectively (1, 0, 0, 1, 1, 0, 0, 1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bb32cc",
   "metadata": {},
   "source": [
    "The Hamming distance between two binary strings of equal length is the number of positions at which the corresponding bits are different. \n",
    "\n",
    "The formula is:\n",
    "\n",
    "Hamming Distance = Σ (bit1 XOR bit2)\n",
    "\n",
    "For 10001011 and 11001111, the Hamming distance is:\n",
    "1 XOR 1 + 0 XOR 1 + 0 XOR 0 + 0 XOR 0 + 1 XOR 0 + 0 XOR 1 + 1 XOR 1 + 1 XOR 1 = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369b435b",
   "metadata": {},
   "source": [
    "The Jaccard index and similarity matching \n",
    "\n",
    "Jaccard Index = (Number of Common Elements) / (Total Number of Unique Elements)\n",
    "\n",
    "Jaccard Index = (3) / (5) = 0.6\n",
    "\n",
    "Similarity Matching Coefficient = (Number of Common Elements) / (Total Number of Non-zero Elements in Either Set)\n",
    "\n",
    "Similarity Matching Coefficient = (3) / (7) = 3/7 ≈ 0.4286"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f4ff709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming Distance: 2\n"
     ]
    }
   ],
   "source": [
    "# Define the two binary strings\n",
    "string1 = \"10001011\"\n",
    "string2 = \"11001111\"\n",
    "\n",
    "# Check that the strings are of the same length\n",
    "if len(string1) != len(string2):\n",
    "    raise ValueError(\"The strings must have the same length\")\n",
    "\n",
    "# Calculate the Hamming distance\n",
    "hamming_distance = sum(bit1 != bit2 for bit1, bit2 in zip(string1, string2))\n",
    "\n",
    "print(f\"Hamming Distance: {hamming_distance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd478eab",
   "metadata": {},
   "source": [
    "# 8. State what is meant by  \"high-dimensional data set\"? Could you offer a few real-life examples? What are the difficulties in using machine learning techniques on a data set with many dimensions? What can be done about it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c3d310",
   "metadata": {},
   "source": [
    "A high-dimensional dataset refers to a dataset with a large number of features or dimensions. \n",
    "\n",
    "Real-life examples include:\n",
    "\n",
    "Medical data with many patient attributes.\n",
    "Image data with pixel values as features.\n",
    "Genomic data with gene expression levels.\n",
    "Difficulties in using machine learning techniques on high-dimensional data include:\n",
    "\n",
    "Increased computational complexity.\n",
    "Overfitting due to the \"curse of dimensionality.\"\n",
    "Difficulty in visualizing data.\n",
    "\n",
    "To address these issues techniques like feature selection, dimensionality reduction (e.g., PCA) and regularization can be applied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d459f3",
   "metadata": {},
   "source": [
    "# 9. Make a few quick notes on: PCA is an acronym for Personal Computer Analysis. 2. Use of vectors 3. Embedded technique\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9280d53",
   "metadata": {},
   "source": [
    "PCA is an acronym for Personal Computer Analysis.\n",
    "\n",
    "Incorrect. PCA stands for Principal Component Analysis a technique for reducing the dimensionality of data while preserving its variance.\n",
    "\n",
    "Use of vectors\n",
    "\n",
    "Vectors are often used to represent data points or features in machine learning. They can be manipulated mathematically to perform various operations.\n",
    "\n",
    "Embedded technique\n",
    "\n",
    "Embedded techniques integrate feature selection within the machine learning model training process where feature importance is assessed during model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2424a662",
   "metadata": {},
   "source": [
    "# 10. Make a comparison between: 1. Sequential backward exclusion vs. sequential forward selection 2. Function selection methods: filter vs. wrapper 3. SMC vs. Jaccard coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2690582",
   "metadata": {},
   "source": [
    "Sequential backward exclusion vs. sequential forward selection\n",
    "\n",
    "Sequential Backward Exclusion: Starts with all features and iteratively removes the least important ones. Prone to underfitting.\n",
    "\n",
    "Sequential Forward Selection: Starts with an empty set of features and iteratively adds the most important ones. Prone to overfitting.\n",
    "\n",
    "Feature selection methods: filter vs. wrapper\n",
    "\n",
    "Filter Methods: Use statistical measures to rank and select features. Fast but may miss feature interactions.\n",
    "\n",
    "Wrapper Methods: Use a specific machine learning model to evaluate feature subsets. Slow but can capture feature interactions.\n",
    "SMC vs. Jaccard coefficient\n",
    "\n",
    "SMC (Simple Matching Coefficient): Considers the number of matching elements divided by the total number of elements in the sets. Sensitive to common and non-common elements.\n",
    "\n",
    "Jaccard Coefficient: Considers the number of common elements divided by the total number of unique elements in the sets. Sensitive to common elements only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572463c3",
   "metadata": {},
   "source": [
    "# Done all 10 questions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04469f9",
   "metadata": {},
   "source": [
    "# Regards,Yashwant"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
